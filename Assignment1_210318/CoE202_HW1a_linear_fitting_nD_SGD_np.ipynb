{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CoE202_HW1a_linear_fitting_nD_SGD_np.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"X2vJ63lCf3Xu"},"source":["# [CoE202] **[Homework1a]** nD Linear fitting with gradient descent"]},{"cell_type":"markdown","metadata":{"id":"PLNtVUc4-K4B"},"source":["In this section, you are going to implement linear regression algorithms."]},{"cell_type":"markdown","metadata":{"id":"QR-xJ3cF5EvV"},"source":["### 0. Importing packages\r\n","\r\n","For this assignment we need Numpy and Matplotlib."]},{"cell_type":"code","metadata":{"id":"BZAWxdotbEwW"},"source":["# Importing packages required for the homework\n","import numpy as np # this is for importing numpy library (and we will use abbreviation np for that)\n","import matplotlib.pyplot as plt # this is for importing matplotlib.pyplot (library for graph plot)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"slU9XZV5oVN0"},"source":["### 1. Linear regression extension\n","\n","For extension, we will consider cases when x is not single dimension but multi dimesional vector. "]},{"cell_type":"code","metadata":{"id":"knunW7ZSqXkJ"},"source":["# data points\n","X = np.array([[0.0, 1.0], [1.0, 1.0], [2.0, 2.0], [3.0, 1.0],  [4.0, 1.0],  [5.0, 3.0]]) \n","y = np.array([0.0, 1.1, 0.9, 3.1, 3.8, 3.1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mjmgXkJ-rnyt"},"source":["# Plotting\n","fig = plt.figure()\n","ax = fig.add_subplot(111, projection='3d')\n","\n","ax.scatter(X[:, 0], X[:, 1], y, label = 'data points') \n","ax.set_xlabel('x0')\n","ax.set_ylabel('x1')\n","ax.set_zlabel('y')\n","plt.legend(loc='upper left')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iPM57lcF_Fvh"},"source":[""]},{"cell_type":"code","metadata":{"id":"Pu8dG002bJ9l"},"source":["def linear_features(x, K):\n","    \"\"\" Computes the feature matrix Phi\n","\n","    Arguments:\n","      x: input data of size N x K\n","      K: dimension of input data x\n","    \n","    Returns:\n","      X_aug: feature matrix of size N x (K + 1)\n","    \"\"\"\n","    X = x.reshape(-1, K)  # 6 x K vector, N=6, D=K\n","    N, _ = X.shape\n","    X_aug = np.hstack([np.ones((N, 1)), X])  # augmented X of size 6 x (K+1)\n","    return X_aug\n","\n","def vectorize_y(y):\n","    y_vec = y.reshape(-1, 1)  # 6 x 1 vector, N=6\n","    return y_vec"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j0SDPjh9uFOE"},"source":["# convert our data into matrix form\n","X_aug = linear_features(X, 2)\n","y_vec = vectorize_y(y)\n","\n","print(X_aug)\n","print(y_vec)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fivzBgkcsupZ"},"source":["### 2. Linear regression using gradient-based optimization algorithms.\n","\n","We will implement 3 different gradient-based optimization algorithms here.\n","1. np_linearfit_gd: Vanilla gradient decent\n","2. np_linearfit_sgd: Stochastic gradient descent \n","3. np_linearfit_sgd_momentum: Stochastic gradient descent with momentum"]},{"cell_type":"code","metadata":{"id":"W8dhkAxlBaOe"},"source":["def np_linearfit(X_aug, y):\r\n","    \"\"\"Compute the coefficients by closed form linear fitting.\r\n","\r\n","    Arguments:\r\n","      X_aug: feature matrix of size N x (K + 1)\r\n","      y: training targets of size N x 1\r\n","    \r\n","    Returns:\r\n","      theta: coefficients of the linear function.\r\n","    \"\"\"\r\n","    theta = (np.linalg.inv(X_aug.T @ X_aug) @ X_aug.T) @ y  # matrix multiplication is also used as \"@\"\r\n","\r\n","    return theta"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dbbzq0VBBcZu"},"source":["def np_linearfit_gd(X_aug, y_vec):\r\n","    \"\"\"Linear fitting using gradient descent.\r\n","\r\n","    Arguments:\r\n","      X_aug: feature matrix of size N x (K + 1)\r\n","      y_vec: training targets of size N x 1\r\n","    \r\n","    Returns:\r\n","      theta: coefficients of the linear function.\r\n","    \"\"\"\r\n","    alpha = 0.01              # learning rate\r\n","    num_iter = 10000          # number of iterations\r\n","    theta = np.zeros((3, 1))  # initialize theta\r\n","    N, _ = X_aug.shape        # the number of input data\r\n","\r\n","    # iteratively apply gradient descent\r\n","    for i in range(num_iter):\r\n","        # [Problem 1] calculate gradient, grad_L from X_aug, theta and y_vec\r\n","        # calculate gradients\r\n","        \r\n","        # Fill out here        grad_L = _________\r\n","\r\n","        # [Problem 2] Update theta using alpha and grad_L\r\n","        # update theta\r\n","        \r\n","        # FIll out here        theta = _________\r\n","  \r\n","    return theta    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"txgEUkR2Bd1f"},"source":["def np_linearfit_sgd(X_aug, y_vec):\r\n","    \"\"\"Linear fitting using stochastic gradient descent (SGD).\r\n","\r\n","    Arguments:\r\n","      X_aug: feature matrix of size N x (K + 1)\r\n","      y_vec: training targets of size N x 1\r\n","    \r\n","    Returns:\r\n","      theta: coefficients of the linear function.\r\n","    \"\"\"\r\n","    alpha = 0.01              # learning rate\r\n","    num_iter = 10000          # number of iterations\r\n","    theta = np.zeros((3, 1))  # initialize theta\r\n","    N, _ = X_aug.shape        # the number of input data\r\n","\r\n","    # iteratively apply gradient descent\r\n","    for i in range(num_iter):\r\n","        # [Problem 3] Shuffle the data order\r\n","        # hint: numpy.random.permutation\r\n","        # fil out here\r\n","\r\n","        for j in range(3): # split 6 data points into 3 mini-batches (2 datapoints per mini-batch)\r\n","          # [Problem 4] calculate gradient, grad_L from X_aug, theta and y_vec from the mini-batch\r\n","          # calculate gradients\r\n","          \r\n","          # Fill out here        grad_L = _________\r\n","\r\n","          # [Problem 5] Update theta using alpha and grad_L using the gradient from the mini-batch\r\n","          # update theta\r\n","          \r\n","          # FIll out here        theta = _________\r\n","  \r\n","\r\n","    return theta   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2pDXDNHCutLK"},"source":["def np_linearfit_sgd_momentum(X_aug, y_vec):\n","    \"\"\"Linear fitting using stochastic gradient descent with momentum.\n","\n","    Arguments:\n","      X_aug: feature matrix of size N x (K + 1)\n","      y_vec: training targets of size N x 1\n","    \n","    Returns:\n","      theta: coefficients of the linear function.\n","    \"\"\"\n","    alpha = 0.01              # learning rate\n","    momentum = 0.9            # momentum\n","    num_iter = 10000          # number of iterations\n","    theta = np.zeros((3, 1))  # initialize theta\n","    N, _ = X_aug.shape        # the number of data\n","\n","    # iteratively apply gradient descent\n","    grad_L = 0  # initialize grad_L to zero\n","    for i in range(num_iter):\n","        # [Problem 6] Shuffle the data order\n","        # hint: numpy.random.permutation\n","        # fil out here _________\n","\n","        for j in range(3): # split 6 data points into 3 mini-batches (2 datapoints per mini-batch)\n","          # [Problem 7] calculate gradient, grad_L from X_aug, theta and y_vec from the mini-batch\n","          # calculate gradients\n","          \n","          # Fill out here        grad_L = _________\n","\n","          # [Problem 8] Update theta using alpha and grad_L using the gradient from the mini-batch (with momentum)\n","          # update theta\n","          \n","          # FIll out here        theta = _________\n","  \n","    return theta           "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ul9WuIwdjRxQ"},"source":["### 3. Testing algorithms."]},{"cell_type":"markdown","metadata":{"id":"Lw1aKLxuPxV3"},"source":["#### 3-0. Let's try running closed form solution (np_linearfit)."]},{"cell_type":"code","metadata":{"id":"tD9cBSRCPiRM"},"source":["# get coefficients \r\n","theta_ml = np_linearfit(X_aug, y_vec)\r\n","print(theta_ml)\r\n","\r\n","# plot results\r\n","xp0 = np.linspace(0, 5, 100) \r\n","xp1 = np.linspace(0, 5, 100)\r\n","Xtest = np.mgrid[0:5:0.25, 0:5:0.25].reshape(2,-1).T  # 10000 x 2 vector for test inputs\r\n","Xtest_aug = np.hstack([np.ones((Xtest.shape[0],1)), Xtest])\r\n","\r\n","prediction = Xtest_aug @ theta_ml\r\n","\r\n","fig = plt.figure()\r\n","ax = fig.add_subplot(111, projection='3d')\r\n","\r\n","ax.scatter(X[:, 0], X[:, 1], y, label = 'data points') \r\n","ax.scatter(Xtest[:, 0], Xtest[:, 1], prediction, label = 'fitted plane', alpha=0.2)\r\n","ax.set_xlabel('x0')\r\n","ax.set_ylabel('x1')\r\n","ax.set_zlabel('y')\r\n","plt.legend(loc='upper left')\r\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4U2MyIdIiv6D"},"source":["#### 3-1. Let's try runing gradient descent (np_linearfit_gd)."]},{"cell_type":"code","metadata":{"id":"3-oPXbn4vPny"},"source":["# get coefficients \n","theta_ml = np_linearfit_gd(X_aug, y_vec)\n","print(theta_ml)\n","\n","# plot results\n","xp0 = np.linspace(0, 5, 100) \n","xp1 = np.linspace(0, 5, 100)\n","Xtest = np.mgrid[0:5:0.25, 0:5:0.25].reshape(2,-1).T  # 10000 x 2 vector for test inputs\n","Xtest_aug = np.hstack([np.ones((Xtest.shape[0],1)), Xtest])\n","\n","prediction = Xtest_aug @ theta_ml\n","\n","fig = plt.figure()\n","ax = fig.add_subplot(111, projection='3d')\n","\n","ax.scatter(X[:, 0], X[:, 1], y, label = 'data points') \n","ax.scatter(Xtest[:, 0], Xtest[:, 1], prediction, label = 'fitted plane', alpha=0.2)\n","ax.set_xlabel('x0')\n","ax.set_ylabel('x1')\n","ax.set_zlabel('y')\n","plt.legend(loc='upper left')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-PB_7WNpi0C_"},"source":["#### 3-2. Let's try runing SGD (np_linearfit_sgd)."]},{"cell_type":"code","metadata":{"id":"s5QLkmJWxYiq"},"source":["# get coefficients \n","theta_ml = np_linearfit_sgd(X_aug, y_vec)\n","print(theta_ml)\n","\n","# plot results\n","xp0 = np.linspace(0, 5, 100) \n","xp1 = np.linspace(0, 5, 100)\n","Xtest = np.mgrid[0:5:0.25, 0:5:0.25].reshape(2,-1).T  # 10000 x 2 vector for test inputs\n","Xtest_aug = np.hstack([np.ones((Xtest.shape[0],1)), Xtest])\n","\n","prediction = Xtest_aug @ theta_ml\n","\n","fig = plt.figure()\n","ax = fig.add_subplot(111, projection='3d')\n","\n","ax.scatter(X[:, 0], X[:, 1], y, label = 'data points') \n","ax.scatter(Xtest[:, 0], Xtest[:, 1], prediction, label = 'fitted plane', alpha=0.2)\n","ax.set_xlabel('x0')\n","ax.set_ylabel('x1')\n","ax.set_zlabel('y')\n","plt.legend(loc='upper left')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6_Ob3Sy1i17F"},"source":["#### 3-3. Let's try runing SGD with momentum (np_linearfit_sgd_momentum)."]},{"cell_type":"code","metadata":{"id":"UKRUfwSSybh_"},"source":["# get coefficients \n","theta_ml = np_linearfit_sgd_momentum(X_aug, y_vec)\n","print(theta_ml)\n","\n","# plot results\n","xp0 = np.linspace(0, 5, 100) \n","xp1 = np.linspace(0, 5, 100)\n","Xtest = np.mgrid[0:5:0.25, 0:5:0.25].reshape(2,-1).T  # 10000 x 2 vector for test inputs\n","Xtest_aug = np.hstack([np.ones((Xtest.shape[0],1)), Xtest])\n","\n","prediction = Xtest_aug @ theta_ml\n","\n","fig = plt.figure()\n","ax = fig.add_subplot(111, projection='3d')\n","\n","ax.scatter(X[:, 0], X[:, 1], y, label = 'data points') \n","ax.scatter(Xtest[:, 0], Xtest[:, 1], prediction, label = 'fitted plane', alpha=0.2)\n","ax.set_xlabel('x0')\n","ax.set_ylabel('x1')\n","ax.set_zlabel('y')\n","plt.legend(loc='upper left')\n","plt.show()"],"execution_count":null,"outputs":[]}]}